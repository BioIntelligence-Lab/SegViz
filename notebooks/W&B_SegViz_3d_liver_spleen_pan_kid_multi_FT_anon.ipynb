{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVHtEqy6Pg0j"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3eIjeNzPg0j",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from monai.utils import first, set_determinism\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    AsDiscreted,\n",
        "    EnsureChannelFirstd,\n",
        "    RandScaleIntensityd,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    RandShiftIntensityd,\n",
        "    Orientationd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    SaveImaged,\n",
        "    RandAffined,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    Invertd,\n",
        "    Resized,\n",
        ")\n",
        "from monai.handlers.utils import from_engine\n",
        "from monai.networks.nets import UNet, BasicUNet\n",
        "from monai.networks.layers import Norm\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "from monai.metrics import DiceMetric, ROCAUCMetric, MSEMetric\n",
        "from monai.networks.utils import copy_model_state\n",
        "from monai.optimizers import generate_param_groups\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import wandb\n",
        "import copy\n",
        "import nibabel as nib\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QovUUOzklUx1"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg7-5RLhIQNX"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8kPOLVIPg0k"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7hO81pBPg0l"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h5uHSK9Pg0l",
        "tags": []
      },
      "outputs": [],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = 'PATH//SegViz/'\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a540_rw-Pg0l"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "Downloads and extracts the dataset.  \n",
        "The dataset comes from http://medicaldecathlon.com/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-HgQODWPg0l",
        "tags": []
      },
      "outputs": [],
      "source": [
        "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
        "md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n",
        "\n",
        "compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n",
        "data_dir_spleen = os.path.join(root_dir, \"Task09_Spleen\")\n",
        "if not os.path.exists(data_dir_spleen):\n",
        "    download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7aSJinPPfjl"
      },
      "outputs": [],
      "source": [
        "resource_liver = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task03_Liver.tar\"\n",
        "md5_liver = \"a90ec6c4aa7f6a3d087205e23d4e6397\"\n",
        "\n",
        "compressed_file_liver = os.path.join(root_dir, \"Task03_Liver.tar\")\n",
        "data_dir_liver = os.path.join(root_dir, \"Task03_Liver\")\n",
        "if not os.path.exists(data_dir_liver):\n",
        "    download_and_extract(resource_liver, compressed_file_liver, root_dir, md5_liver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJwBBKGdHsUb"
      },
      "outputs": [],
      "source": [
        "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task07_Pancreas.tar\"\n",
        "md5_pan = \"4f7080cfca169fa8066d17ce6eb061e4\"\n",
        "\n",
        "compressed_file_pan = os.path.join(root_dir, \"Task07_Pancreas.tar\")\n",
        "data_dir_pan = os.path.join(root_dir, \"Task07_Pancreas\")\n",
        "if not os.path.exists(data_dir_pan):\n",
        "    download_and_extract(resource, compressed_file_pan, root_dir, md5_pan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI6oo1sBPg0m"
      },
      "source": [
        "## Set MSD Spleen dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGNMKPSzPg0m"
      },
      "outputs": [],
      "source": [
        "train_images = sorted(\n",
        "    glob.glob(os.path.join(data_dir_spleen, \"imagesTr\", \"*.nii.gz\")))\n",
        "train_labels = sorted(\n",
        "    glob.glob(os.path.join(data_dir_spleen, \"labelsTr\", \"*.nii.gz\")))\n",
        "data_dicts_spleen = [\n",
        "    {\"image\": image_name, \"label\": label_name}\n",
        "    for image_name, label_name in zip(train_images, train_labels)\n",
        "]\n",
        "train_files_spleen, val_files_spleen = data_dicts_spleen[:-9], data_dicts_spleen[-9:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1Q681AXQ458"
      },
      "source": [
        "## Set MSD Liver dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCVoDfK2QINp"
      },
      "outputs": [],
      "source": [
        "train_images = sorted(\n",
        "    glob.glob(os.path.join(data_dir_liver, \"imagesTr\", \"*.nii.gz\")))\n",
        "train_labels = sorted(\n",
        "    glob.glob(os.path.join(data_dir_liver, \"labelsTr\", \"*.nii.gz\")))\n",
        "data_dicts_liver = [\n",
        "    {\"image\": image_name, \"label\": label_name}\n",
        "    for image_name, label_name in zip(train_images, train_labels)\n",
        "]\n",
        "train_files_liver, val_files_liver = data_dicts_liver[:-25], data_dicts_liver[-25:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q3_udoYH7ru"
      },
      "source": [
        "# Set MSD Pancreas dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F20G-zH5LR"
      },
      "outputs": [],
      "source": [
        "train_images = sorted(\n",
        "    glob.glob(os.path.join(data_dir_pan, \"imagesTr\", \"*.nii.gz\")))\n",
        "train_labels = sorted(\n",
        "    glob.glob(os.path.join(data_dir_pan, \"labelsTr\", \"*.nii.gz\")))\n",
        "data_dicts_pan = [\n",
        "    {\"image\": image_name, \"label\": label_name}\n",
        "    for image_name, label_name in zip(train_images, train_labels)\n",
        "]\n",
        "train_files_pan, val_files_pan = data_dicts_pan[:-56], data_dicts_pan[-56:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmLhWSmYIQNZ"
      },
      "source": [
        "# Set Kits dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V_3RMfMIQNZ"
      },
      "outputs": [],
      "source": [
        "data_dir_kits = 'PATH//Task040_KiTS'\n",
        "train_images = sorted(\n",
        "    glob.glob(os.path.join(data_dir_kits, \"imagesTr\", \"*.nii.gz\")))\n",
        "train_labels = sorted(\n",
        "    glob.glob(os.path.join(data_dir_kits, \"labelsTr\", \"*.nii.gz\")))\n",
        "data_dicts_kits = [\n",
        "    {\"image\": image_name, \"label\": label_name}\n",
        "    for image_name, label_name in zip(train_images, train_labels)\n",
        "]\n",
        "train_files_kits, val_files_kits = data_dicts_kits[:-42], data_dicts_kits[-42:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKiSne_S5U-H"
      },
      "source": [
        "## Convert Multilabel to single label for liver and pancreas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPjmx8ndkSBh"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "for image_path in sorted(glob.glob(os.path.join(data_dir_liver, \"labelsTr\", \"*.nii.gz\"))):\n",
        "    image_file = nib.load(image_path)\n",
        "    image_file_array = nib.load(image_path).get_fdata()\n",
        "    image_file_array[image_file_array > 1 ] = 1\n",
        "    image_file_final = nib.Nifti1Image(image_file_array, image_file.affine)\n",
        "    nib.save(image_file_final , image_path)\n",
        "\n",
        "for image_path in sorted(glob.glob(os.path.join(data_dir_pan, \"labelsTr\", \"*.nii.gz\"))):\n",
        "    image_file = nib.load(image_path)\n",
        "    image_file_array = nib.load(image_path).get_fdata()\n",
        "    image_file_array[image_file_array > 1 ] = 1\n",
        "    image_file_final = nib.Nifti1Image(image_file_array, image_file.affine)\n",
        "    nib.save(image_file_final , image_path)\n",
        "\n",
        "for image_path in sorted(glob.glob(os.path.join(data_dir_kits, \"labelsTr\", \"*.nii.gz\"))):\n",
        "    image_file = nib.load(image_path)\n",
        "    image_file_array = nib.load(image_path).get_fdata()\n",
        "    image_file_array[image_file_array > 1 ] = 1\n",
        "    image_file_final = nib.Nifti1Image(image_file_array, image_file.affine)\n",
        "    nib.save(image_file_final , image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMK0hwYxPg0m"
      },
      "source": [
        "## Set deterministic training for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VfXGpEAPg0m"
      },
      "outputs": [],
      "source": [
        "set_determinism(seed=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y_VXPUFPg0m"
      },
      "source": [
        "## Setup transforms for training and validation\n",
        "\n",
        "Here we use several transforms to augment the dataset:\n",
        "1. `LoadImaged` loads the spleen CT images and labels from NIfTI format files.\n",
        "1. `EnsureChannelFirstd` ensures the original data to construct \"channel first\" shape.\n",
        "1. `Orientationd` unifies the data orientation based on the affine matrix.\n",
        "1. `Spacingd` adjusts the spacing by `pixdim=(1.5, 1.5, 2.)` based on the affine matrix.\n",
        "1. `ScaleIntensityRanged` extracts intensity range [-57, 164] and scales to [0, 1].\n",
        "1. `CropForegroundd` removes all zero borders to focus on the valid body area of the images and labels.\n",
        "1. `RandCropByPosNegLabeld` randomly crop patch samples from big image based on pos / neg ratio.  \n",
        "The image centers of negative samples must be in valid body area.\n",
        "1. `RandAffined` efficiently performs `rotate`, `scale`, `shear`, `translate`, etc. together based on PyTorch affine transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUNPr9J5Pg0m"
      },
      "outputs": [],
      "source": [
        "train_transforms_spleen = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"], a_min=-57, a_max=164,\n",
        "            b_min=0.0, b_max=1.0, clip=True,\n",
        "        ),\n",
        "        Resized(keys=[\"image\"], spatial_size=(256,256,128)),   \n",
        "        Resized(keys=[\"label\"], spatial_size=(256,256,128), mode='nearest'),   \n",
        "        # CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
        "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
        "        RandCropByPosNegLabeld(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            label_key=\"label\",\n",
        "            spatial_size=(128,128,32),\n",
        "            pos=1,\n",
        "            neg=1,\n",
        "            num_samples=4,\n",
        "            image_key=\"image\",\n",
        "            image_threshold=0,\n",
        "        ),\n",
        "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
        "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
        "        # user can also add other random transforms\n",
        "        RandAffined(\n",
        "            keys=['image', 'label'],\n",
        "            mode=('bilinear', 'nearest'),\n",
        "            prob=1.0, spatial_size=(128,128,32),\n",
        "            rotate_range=(0, 0, np.pi/15),\n",
        "            scale_range=(0.1, 0.1, 0.1)),\n",
        "    ]\n",
        ")\n",
        "val_transform_spleen = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"], a_min=-57, a_max=164,\n",
        "            b_min=0.0, b_max=1.0, clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
        "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transforms_liver = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"], a_min=-200, a_max=200,\n",
        "            b_min=0.0, b_max=1.0, clip=True,\n",
        "        ),\n",
        "        Resized(keys=[\"image\"], spatial_size=(256,256,128)),   \n",
        "        Resized(keys=[\"label\"], spatial_size=(256,256,128), mode='nearest'),   \n",
        "        # CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
        "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
        "        RandCropByPosNegLabeld(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            label_key=\"label\",\n",
        "            spatial_size=(128,128,32),\n",
        "            pos=1,\n",
        "            neg=1,\n",
        "            num_samples=4,\n",
        "            image_key=\"image\",\n",
        "            image_threshold=0,\n",
        "        ),\n",
        "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
        "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
        "        # user can also add other random transforms\n",
        "        RandAffined(\n",
        "            keys=['image', 'label'],\n",
        "            mode=('bilinear', 'nearest'),\n",
        "            prob=1.0, spatial_size=(128,128,32),\n",
        "            rotate_range=(0, 0, np.pi/15),\n",
        "            scale_range=(0.1, 0.1, 0.1)),\n",
        "    ]\n",
        ")\n",
        "val_transform_liver = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"], a_min=-200, a_max=200,\n",
        "            b_min=0.0, b_max=1.0, clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
        "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transforms_pan = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"], a_min=-87, a_max=199,\n",
        "            b_min=0.0, b_max=1.0, clip=True,\n",
        "        ),\n",
        "        Resized(keys=[\"image\"], spatial_size=(256,256,128)),   \n",
        "        Resized(keys=[\"label\"], spatial_size=(256,256,128), mode='nearest'),   \n",
        "        # CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
        "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
        "        RandCropByPosNegLabeld(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            label_key=\"label\",\n",
        "            spatial_size=(128,128,32),\n",
        "            pos=1,\n",
        "            neg=1,\n",
        "            num_samples=4,\n",
        "            image_key=\"image\",\n",
        "            image_threshold=0,\n",
        "        ),\n",
        "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
        "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
        "        # user can also add other random transforms\n",
        "        RandAffined(\n",
        "            keys=['image', 'label'],\n",
        "            mode=('bilinear', 'nearest'),\n",
        "            prob=1.0, spatial_size=(128,128,32),\n",
        "            rotate_range=(0, 0, np.pi/15),\n",
        "            scale_range=(0.1, 0.1, 0.1)),\n",
        "    ]\n",
        ")\n",
        "val_transform_pan = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"], a_min=-87, a_max=199,\n",
        "            b_min=0.0, b_max=1.0, clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
        "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transforms_kits = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"], a_min=-79, a_max=304,\n",
        "            b_min=0.0, b_max=1.0, clip=True,\n",
        "        ),\n",
        "        Resized(keys=[\"image\"], spatial_size=(256,256,128)),   \n",
        "        Resized(keys=[\"label\"], spatial_size=(256,256,128), mode='nearest'),   \n",
        "        # CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
        "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
        "        RandCropByPosNegLabeld(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            label_key=\"label\",\n",
        "            spatial_size=(128,128,32),\n",
        "            pos=1,\n",
        "            neg=1,\n",
        "            num_samples=4,\n",
        "            image_key=\"image\",\n",
        "            image_threshold=0,\n",
        "        ),\n",
        "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
        "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
        "        # user can also add other random transforms\n",
        "        RandAffined(\n",
        "            keys=['image', 'label'],\n",
        "            mode=('bilinear', 'nearest'),\n",
        "            prob=1.0, spatial_size=(128,128,32),\n",
        "            rotate_range=(0, 0, np.pi/15),\n",
        "            scale_range=(0.1, 0.1, 0.1)),\n",
        "    ]\n",
        ")\n",
        "val_transform_kits = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"], a_min=-79, a_max=304,\n",
        "            b_min=0.0, b_max=1.0, clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
        "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMmJmbGvPg0n"
      },
      "source": [
        "## Check transforms in DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFK7donFPg0n",
        "tags": []
      },
      "outputs": [],
      "source": [
        "check_ds = Dataset(data=train_files_spleen, transform=train_transforms_spleen)\n",
        "check_loader = DataLoader(check_ds, batch_size=1)\n",
        "check_data = first(check_loader)\n",
        "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
        "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
        "print(np.unique(label))\n",
        "\n",
        "# plot the slice [:, :, 80]\n",
        "plt.figure(\"check\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"image\")\n",
        "plt.imshow(image[:, :, 16], cmap=\"gray\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"label\")\n",
        "plt.imshow(label[:, :, 16])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLF_5fEbVWQT"
      },
      "outputs": [],
      "source": [
        "check_ds = Dataset(data=train_files_liver, transform=train_transforms_liver)\n",
        "check_loader = DataLoader(check_ds, batch_size=1)\n",
        "check_data = first(check_loader)\n",
        "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
        "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
        "print(np.unique(label))\n",
        "\n",
        "# plot the slice [:, :, 80]\n",
        "plt.figure(\"check\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"image\")\n",
        "plt.imshow(image[:, :, 16], cmap=\"gray\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"label\")\n",
        "plt.imshow(label[:, :, 16])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMOkEIV3Ihsv"
      },
      "outputs": [],
      "source": [
        "check_ds = Dataset(data=train_files_pan, transform=train_transforms_pan)\n",
        "check_loader = DataLoader(check_ds, batch_size=1)\n",
        "check_data = first(check_loader)\n",
        "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
        "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
        "print(np.unique(label))\n",
        "\n",
        "# plot the slice [:, :, 80]\n",
        "plt.figure(\"check\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"image\")\n",
        "plt.imshow(image[:, :, 16], cmap=\"gray\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"label\")\n",
        "plt.imshow(label[:, :, 16])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5lk4-lnIQNa"
      },
      "outputs": [],
      "source": [
        "check_ds = Dataset(data=train_files_kits, transform=train_transforms_kits)\n",
        "check_loader = DataLoader(check_ds, batch_size=1)\n",
        "check_data = first(check_loader)\n",
        "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
        "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
        "print(np.unique(label))\n",
        "\n",
        "# plot the slice [:, :, 80]\n",
        "plt.figure(\"check\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"image\")\n",
        "plt.imshow(image[:, :, 16], cmap=\"gray\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"label\")\n",
        "plt.imshow(label[:, :, 16])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTxtC2ycjSpH"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # data\n",
        "    \"cache_rate_spleen\": 1.0,\n",
        "    \"num_workers\": 0,\n",
        "\n",
        "\n",
        "    # train settings\n",
        "    \"train_batch_size\": 2,\n",
        "    \"val_batch_size\": 1,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"max_epochs\": 150,\n",
        "    \"val_interval\": 2, # check validation score after n epochs\n",
        "    \"lr_scheduler\": \"cosine_decay\", # just to keep track\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Unet model (you can even use nested dictionary and this will be handled by W&B automatically)\n",
        "    \"model_type_spleen\": \"unet\", # just to keep track\n",
        "    \"model_type_liver\": \"unet\",\n",
        "    \"model_type_pan\": \"unet\",\n",
        "    \"model_type_kits\": \"unet\",\n",
        "    \"model_params_spleen\": dict(spatial_dims=3,\n",
        "                  in_channels=1,\n",
        "                  out_channels=2,\n",
        "                  channels=(16, 32, 64, 128, 256),\n",
        "                  strides=(2, 2, 2, 2),\n",
        "                  num_res_units=2,\n",
        "                  norm=Norm.BATCH,),\n",
        "    \"model_params_liver\": dict(spatial_dims=3,\n",
        "                      in_channels=1,\n",
        "                      out_channels=2,\n",
        "                      channels=(16, 32, 64, 128, 256),\n",
        "                      strides=(2, 2, 2, 2),\n",
        "                      num_res_units=2,\n",
        "                      norm=Norm.BATCH,),\n",
        "    \"model_params_pan\": dict(spatial_dims=3,\n",
        "                in_channels=1,\n",
        "                out_channels=2,\n",
        "                channels=(16, 32, 64, 128, 256),\n",
        "                strides=(2, 2, 2, 2),\n",
        "                num_res_units=2,\n",
        "                norm=Norm.BATCH,),\n",
        "    \"model_params_kits\": dict(spatial_dims=3,\n",
        "                in_channels=1,\n",
        "                out_channels=2,\n",
        "                channels=(16, 32, 64, 128, 256),\n",
        "                strides=(2, 2, 2, 2),\n",
        "                num_res_units=2,\n",
        "                norm=Norm.BATCH,),\n",
        "    # data\n",
        "    \"cache_rate_liver\": 0.4,\n",
        "    \"cache_rate_pan\": 0.4,\n",
        "    \"cache_rate_kits\": 0.4,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1vDaw4aPg0n"
      },
      "source": [
        "## Define CacheDataset and DataLoader for training and validation\n",
        "\n",
        "Here we use CacheDataset to accelerate training and validation process, it's 10x faster than the regular Dataset.  \n",
        "To achieve best performance, set `cache_rate=1.0` to cache all the data, if memory is not enough, set lower value.  \n",
        "Users can also set `cache_num` instead of `cache_rate`, will use the minimum value of the 2 settings.  \n",
        "And set `num_workers` to enable multi-threads during caching.  \n",
        "If want to to try the regular Dataset, just change to use the commented code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiKh-dJvj0Q2"
      },
      "outputs": [],
      "source": [
        "train_ds_spleen = CacheDataset(\n",
        "    data=train_files_spleen, transform=train_transforms_spleen,\n",
        "    cache_rate=config['cache_rate_spleen'], num_workers=config['num_workers'])\n",
        "# train_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "\n",
        "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
        "# to generate 2 x 4 images for network training\n",
        "train_loader_spleen = DataLoader(train_ds_spleen, batch_size=config['train_batch_size'], shuffle=True, num_workers=config['num_workers'])\n",
        "\n",
        "val_ds_spleen = CacheDataset(\n",
        "    data=val_files_spleen, transform=val_transform_spleen, cache_rate=config['cache_rate_spleen'], num_workers=config['num_workers'])\n",
        "# val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader_spleen = DataLoader(val_ds_spleen, batch_size=config['val_batch_size'], num_workers=config['num_workers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rViCTXO-kRx0"
      },
      "outputs": [],
      "source": [
        "train_ds_liver = CacheDataset(\n",
        "    data=train_files_liver, transform=train_transforms_liver,\n",
        "    cache_rate=config['cache_rate_liver'], num_workers=config['num_workers'])\n",
        "# train_ds_liver = Dataset(data=train_files_liver, transform=train_transforms_liver)\n",
        "\n",
        "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
        "# to generate 2 x 4 images for network training\n",
        "train_loader_liver = DataLoader(train_ds_liver, batch_size=config['train_batch_size'], shuffle=True, num_workers=config['num_workers'])\n",
        "\n",
        "val_ds_liver = CacheDataset(\n",
        "    data=val_files_liver, transform=val_transform_liver, cache_rate=config['cache_rate_liver'], num_workers=config['num_workers'])\n",
        "# val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader_liver = DataLoader(val_ds_liver, batch_size=config['val_batch_size'], num_workers=config['num_workers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tXvw9FOIQNa"
      },
      "outputs": [],
      "source": [
        "train_ds_pan = CacheDataset(\n",
        "    data=train_files_pan, transform=train_transforms_pan,\n",
        "    cache_rate=config['cache_rate_pan'], num_workers=config['num_workers'])\n",
        "# train_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "\n",
        "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
        "# to generate 2 x 4 images for network training\n",
        "train_loader_pan = DataLoader(train_ds_pan, batch_size=config['train_batch_size'], shuffle=True, num_workers=config['num_workers'])\n",
        "\n",
        "val_ds_pan = CacheDataset(\n",
        "    data=val_files_pan, transform=val_transform_pan, cache_rate=config['cache_rate_pan'], num_workers=config['num_workers'])\n",
        "# val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader_pan = DataLoader(val_ds_pan, batch_size=config['val_batch_size'], num_workers=config['num_workers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWwwb6VDI-5-"
      },
      "outputs": [],
      "source": [
        "train_ds_kits = CacheDataset(\n",
        "    data=train_files_kits, transform=train_transforms_kits,\n",
        "    cache_rate=config['cache_rate_kits'], num_workers=config['num_workers'])\n",
        "# train_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "\n",
        "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
        "# to generate 2 x 4 images for network training\n",
        "train_loader_kits = DataLoader(train_ds_kits, batch_size=config['train_batch_size'], shuffle=True, num_workers=config['num_workers'])\n",
        "\n",
        "val_ds_kits = CacheDataset(\n",
        "    data=val_files_kits, transform=val_transform_kits, cache_rate=config['cache_rate_kits'], num_workers=config['num_workers'])\n",
        "# val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader_kits = DataLoader(val_ds_kits, batch_size=config['val_batch_size'], num_workers=config['num_workers'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzC3W1bCPg0n"
      },
      "source": [
        "## Create Model, Loss, Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzqANOZHkzbm"
      },
      "outputs": [],
      "source": [
        "# standard PyTorch program style: create UNet, DiceLoss and Adam optimizer\n",
        "model_spleen = UNet(**config['model_params_spleen']).to(device)\n",
        "model_liver = UNet(**config['model_params_liver']).to(device)\n",
        "model_pan = UNet(**config['model_params_pan']).to(device)\n",
        "model_kits = UNet(**config['model_params_kits']).to(device)\n",
        "\n",
        "\n",
        "loss_function_spleen = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "loss_function_liver = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "loss_function_pan = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "loss_function_kits = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "\n",
        "optimizer_spleen = torch.optim.Adam(model_spleen.parameters(), lr=config['learning_rate'])\n",
        "optimizer_liver = torch.optim.Adam(model_liver.parameters(), lr=config['learning_rate'])\n",
        "optimizer_pan = torch.optim.Adam(model_pan.parameters(), lr=config['learning_rate'])\n",
        "optimizer_kits = torch.optim.Adam(model_kits.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "dice_metric_spleen = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "dice_metric_liver = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "dice_metric_pan = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "dice_metric_kits = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "\n",
        "scheduler_spleen = CosineAnnealingLR(optimizer_spleen, T_max=config['max_epochs'], eta_min=1e-9)\n",
        "scheduler_liver = CosineAnnealingLR(optimizer_liver, T_max=config['max_epochs'], eta_min=1e-9)\n",
        "scheduler_pan = CosineAnnealingLR(optimizer_pan, T_max=config['max_epochs'], eta_min=1e-9)\n",
        "scheduler_kits = CosineAnnealingLR(optimizer_kits, T_max=config['max_epochs'], eta_min=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWIlrKUdIQNb"
      },
      "outputs": [],
      "source": [
        "# üêù initialize a wandb run\n",
        "wandb.init(\n",
        "    project=\"SegViz_LSPK\",\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmOtJ4uXPg0n"
      },
      "source": [
        "## Execute a typical PyTorch training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoyyI_QmPg0n",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# üêù log gradients of the model to wandb\n",
        "wandb.watch(model_spleen, log_freq=100)\n",
        "wandb.watch(model_liver, log_freq=100)\n",
        "wandb.watch(model_pan, log_freq=100)\n",
        "wandb.watch(model_kits, log_freq=100)\n",
        "\n",
        "\n",
        "max_epochs = 500\n",
        "val_interval = 2\n",
        "best_metric = -1\n",
        "best_metric_spleen = -1\n",
        "best_metric_liver = -1\n",
        "best_metric_pan = -1\n",
        "best_metric_kits = -1\n",
        "\n",
        "best_metric_epoch = -1\n",
        "best_metric_epoch_spleen = -1\n",
        "best_metric_epoch_liver = -1\n",
        "best_metric_epoch_pan = -1\n",
        "best_metric_epoch_kits = -1\n",
        "\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "\n",
        "epoch_loss_values_spleen = []\n",
        "metric_values_spleen = []\n",
        "\n",
        "epoch_loss_values_liver = []\n",
        "metric_values_liver = []\n",
        "\n",
        "epoch_loss_values_pan = []\n",
        "metric_values_pan = []\n",
        "\n",
        "epoch_loss_values_kits = []\n",
        "metric_values_kits = []\n",
        "\n",
        "post_pred_spleen = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label_spleen = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "post_pred_liver = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label_liver = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "post_pred_pan = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label_pan = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "post_pred_kits = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label_kits = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "  epoch_loss_spleen = 0\n",
        "  epoch_loss_liver = 0\n",
        "  epoch_loss_pan = 0\n",
        "  epoch_loss_kits = 0\n",
        "\n",
        "  step_0 = 0\n",
        "  step_1 = 0\n",
        "  \n",
        "  # For one epoch\n",
        "  \n",
        "  print(\"-\" * 10)\n",
        "  print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "  \n",
        "  model_spleen.train()\n",
        "  model_liver.train()\n",
        "  model_pan.train()\n",
        "  model_kits.train()\n",
        "  \n",
        "  # One forward pass of the spleen data through the spleen UNet\n",
        "  for batch_data_spleen in train_loader_spleen:\n",
        "      step_0 += 1\n",
        "      inputs_spleen, labels_spleen = (\n",
        "          batch_data_spleen[\"image\"].to(device),\n",
        "          batch_data_spleen[\"label\"].to(device),\n",
        "      )\n",
        "      optimizer_spleen.zero_grad()\n",
        "      outputs_spleen = model_spleen(inputs_spleen)\n",
        "      \n",
        "      loss_spleen = loss_function_spleen(outputs_spleen, labels_spleen)\n",
        "      loss_spleen.backward()\n",
        "      \n",
        "      optimizer_spleen.step()\n",
        "      epoch_loss_spleen += loss_spleen.item()\n",
        "      print(\n",
        "          f\"{step_0}/{len(train_ds_spleen) // train_loader_spleen.batch_size}, \"\n",
        "          f\"train_loss: {loss_spleen.item():.4f}\")\n",
        "      wandb.log({\"train/loss spleen\": loss_spleen.item()})\n",
        "  epoch_loss_spleen /= step_0\n",
        "  epoch_loss_values_spleen.append(epoch_loss_spleen)\n",
        "  print(f\"epoch {epoch + 1} average loss spleen: {epoch_loss_spleen:.4f}\")\n",
        "\n",
        "\n",
        "  # One forward pass of the liver data through the liver UNet\n",
        "  for batch_data_liver in train_loader_liver:\n",
        "      step_1 += 1\n",
        "      inputs_liver, labels_liver = (\n",
        "          batch_data_liver[\"image\"].to(device),\n",
        "          batch_data_liver[\"label\"].to(device),\n",
        "      )\n",
        "      optimizer_liver.zero_grad()\n",
        "      outputs_liver = model_liver(inputs_liver)\n",
        "      loss_liver = loss_function_liver(outputs_liver, labels_liver)\n",
        "      loss_liver.backward()\n",
        "      optimizer_liver.step()\n",
        "      epoch_loss_liver += loss_liver.item()\n",
        "      print(\n",
        "          f\"{step_1}/{len(train_ds_liver) // train_loader_liver.batch_size}, \"\n",
        "          f\"train_loss: {loss_liver.item():.4f}\")\n",
        "      wandb.log({\"train/loss liver\": loss_liver.item()})\n",
        "  epoch_loss_liver /= step_1\n",
        "  epoch_loss_values_liver.append(epoch_loss_liver)\n",
        "  print(f\"epoch {epoch + 1} average loss liver: {epoch_loss_liver:.4f}\")\n",
        "\n",
        "   # One forward pass of the pancreas data through the Pancreas UNet\n",
        "  for batch_data_pan in train_loader_pan:\n",
        "      step_1 += 1\n",
        "      inputs_pan, labels_pan = (\n",
        "          batch_data_pan[\"image\"].to(device),\n",
        "          batch_data_pan[\"label\"].to(device),\n",
        "      )\n",
        "      optimizer_pan.zero_grad()\n",
        "      outputs_pan = model_pan(inputs_pan)\n",
        "      loss_pan = loss_function_pan(outputs_pan, labels_pan)\n",
        "      loss_pan.backward()\n",
        "      optimizer_pan.step()\n",
        "      epoch_loss_pan += loss_pan.item()\n",
        "      print(\n",
        "          f\"{step_1}/{len(train_ds_pan) // train_loader_pan.batch_size}, \"\n",
        "          f\"train_loss: {loss_pan.item():.4f}\")\n",
        "      wandb.log({\"train/loss pancreas\": loss_pan.item()})\n",
        "  epoch_loss_pan /= step_1\n",
        "  epoch_loss_values_pan.append(epoch_loss_pan)\n",
        "  print(f\"epoch {epoch + 1} average loss pancreas: {epoch_loss_pan:.4f}\")\n",
        "\n",
        "  # One forward pass of the kidneys data through the KITS UNet\n",
        "  for batch_data_kits in train_loader_kits:\n",
        "      step_1 += 1\n",
        "      inputs_kits, labels_kits = (\n",
        "          batch_data_kits[\"image\"].to(device),\n",
        "          batch_data_kits[\"label\"].to(device),\n",
        "      )\n",
        "      optimizer_kits.zero_grad()\n",
        "      outputs_kits = model_kits(inputs_kits)\n",
        "      loss_kits = loss_function_kits(outputs_kits, labels_kits)\n",
        "      loss_kits.backward()\n",
        "      optimizer_kits.step()\n",
        "      epoch_loss_kits += loss_kits.item()\n",
        "      print(\n",
        "          f\"{step_1}/{len(train_ds_kits) // train_loader_kits.batch_size}, \"\n",
        "          f\"train_loss: {loss_kits.item():.4f}\")\n",
        "      wandb.log({\"train/loss kidneys\": loss_kits.item()})\n",
        "  epoch_loss_kits /= step_1\n",
        "  epoch_loss_values_kits.append(epoch_loss_kits)\n",
        "  print(f\"epoch {epoch + 1} average loss kidneys: {epoch_loss_kits:.4f}\")\n",
        "  \n",
        "  scheduler_spleen.step()\n",
        "  scheduler_liver.step()\n",
        "  scheduler_pan.step()\n",
        "  scheduler_kits.step()\n",
        "\n",
        "  wandb.log({\"train/loss_epoch spleen\": epoch_loss_spleen})\n",
        "    # üêù log learning rate after each epoch to wandb\n",
        "  wandb.log({\"learning_rate spleen\": scheduler_spleen.get_last_lr()[0]})\n",
        "\n",
        "  wandb.log({\"train/loss_epoch liver\": epoch_loss_liver})\n",
        "      # üêù log learning rate after each epoch to wandb\n",
        "  wandb.log({\"learning_rate liver\": scheduler_liver.get_last_lr()[0]})\n",
        "\n",
        "  wandb.log({\"train/loss_epoch pan\": epoch_loss_pan})\n",
        "      # üêù log learning rate after each epoch to wandb\n",
        "  wandb.log({\"learning_rate pan\": scheduler_pan.get_last_lr()[0]})\n",
        "\n",
        "  wandb.log({\"train/loss_epoch kidneys\": epoch_loss_kits})\n",
        "      # üêù log learning rate after each epoch to wandb\n",
        "  wandb.log({\"learning_rate kidneys\": scheduler_kits.get_last_lr()[0]})\n",
        "\n",
        "  # Store weights before aggregation strategy\n",
        "  \n",
        "  if epoch % 10 == 0:\n",
        "    meta_model = []\n",
        "    weights_liver = []\n",
        "    weights_spleen = []\n",
        "    weights_pan = []\n",
        "    weights_kits = []\n",
        "    # Aggregate weights\n",
        "    for name, param in model_liver.named_parameters():\n",
        "      if not \"model.2\" in name:\n",
        "        weights_liver.append(param)\n",
        "\n",
        "    for name, param in model_spleen.named_parameters():\n",
        "      if not \"model.2\" in name:\n",
        "        weights_spleen.append(param)\n",
        "\n",
        "    for name, param in model_pan.named_parameters():\n",
        "      if not \"model.2\" in name:\n",
        "        weights_pan.append(param)\n",
        "    \n",
        "    for name, param in model_kits.named_parameters():\n",
        "      if not \"model.2\" in name:\n",
        "        weights_kits.append(param)\n",
        "\n",
        "    for weight_spleen, weight_liver, weight_pan, weight_kit in zip(weights_spleen,weights_liver,weights_pan, weights_kits):\n",
        "      meta_model.append((weight_spleen + weight_liver + weight_pan + weight_kit)/4)   # Change aggregation strategy\n",
        "\n",
        "    for index_old, param_old in enumerate(model_spleen.parameters()):\n",
        "      for index_new, param_new in enumerate(meta_model):\n",
        "        param_old = param_new\n",
        "\n",
        "    for index_old, param_old in enumerate(model_liver.parameters()):\n",
        "      for index_new, param_new in enumerate(meta_model):\n",
        "        param_old = param_new\n",
        "    \n",
        "    for index_old, param_old in enumerate(model_pan.parameters()):\n",
        "      for index_new, param_new in enumerate(meta_model):\n",
        "        param_old = param_new\n",
        "    \n",
        "    for index_old, param_old in enumerate(model_kits.parameters()):\n",
        "      for index_new, param_new in enumerate(meta_model):\n",
        "        param_old = param_new\n",
        "\n",
        "  # Validation \n",
        "  if (epoch + 1) % val_interval == 0:\n",
        "    model_spleen.eval()\n",
        "    model_liver.eval()\n",
        "    model_pan.eval()\n",
        "    model_kits.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Validation forward spleen\n",
        "        for val_data_spleen in val_loader_spleen:\n",
        "            val_inputs_spleen, val_labels_spleen = (\n",
        "                val_data_spleen[\"image\"].to(device),\n",
        "                val_data_spleen[\"label\"].to(device),\n",
        "            )\n",
        "            roi_size = (160, 160, 160)\n",
        "            sw_batch_size = 4\n",
        "            val_outputs_spleen = sliding_window_inference(\n",
        "                val_inputs_spleen, roi_size, sw_batch_size, model_spleen)\n",
        "            val_outputs_spleen = [post_pred_spleen(i) for i in decollate_batch(val_outputs_spleen)]\n",
        "            val_labels_spleen = [post_label_spleen(i) for i in decollate_batch(val_labels_spleen)]\n",
        "            # compute metric for current iteration\n",
        "            dice_metric_spleen(y_pred=val_outputs_spleen, y=val_labels_spleen)\n",
        "\n",
        "        # aggregate the final mean dice result\n",
        "        metric_spleen = dice_metric_spleen.aggregate().item()\n",
        "        wandb.log({\"val/dice_metric spleen\": metric_spleen})\n",
        "        scheduler_spleen.step(metric_spleen)\n",
        "        # reset the status for next validation round\n",
        "        dice_metric_spleen.reset()\n",
        "\n",
        "        metric_values_spleen.append(metric_spleen)\n",
        "        if metric_spleen > best_metric_spleen:\n",
        "            best_metric_spleen = metric_spleen\n",
        "            best_metric_epoch_spleen = epoch + 1\n",
        "        #     torch.save(model_spleen.state_dict(), os.path.join(\n",
        "        #         root_dir, \"best_metric_model_spleen_128.pth\"))\n",
        "        #     print(\"saved new best metric model for spleen dataset\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current mean dice for spleen: {metric_spleen:.4f}\"\n",
        "                f\"\\nbest mean dice for spleen: {best_metric_spleen:.4f} \"\n",
        "                f\"at epoch: {best_metric_epoch_spleen}\"\n",
        "            )\n",
        "\n",
        "\n",
        "        # Validation forward Liver \n",
        "        for val_data_liver in val_loader_liver:\n",
        "            val_inputs_liver, val_labels_liver = (\n",
        "                val_data_liver[\"image\"].to(device),\n",
        "                val_data_liver[\"label\"].to(device),\n",
        "            )\n",
        "            roi_size = (160, 160, 160)\n",
        "            sw_batch_size = 4\n",
        "            val_outputs_liver = sliding_window_inference(\n",
        "                val_inputs_liver, roi_size, sw_batch_size, model_liver)\n",
        "            val_outputs_liver = [post_pred_liver(i) for i in decollate_batch(val_outputs_liver)]\n",
        "            val_labels_liver = [post_label_liver(i) for i in decollate_batch(val_labels_liver)]\n",
        "            # compute metric for current iteration\n",
        "            dice_metric_liver(y_pred=val_outputs_liver, y=val_labels_liver)\n",
        "\n",
        "        # aggregate the final mean dice result\n",
        "        metric_liver = dice_metric_liver.aggregate().item()\n",
        "        wandb.log({\"val/dice_metric liver\": metric_liver})\n",
        "\n",
        "        scheduler_liver.step(metric_liver)\n",
        "        # reset the status for next validation round\n",
        "        dice_metric_liver.reset()\n",
        "\n",
        "        metric_values_liver.append(metric_liver)\n",
        "        if metric_liver > best_metric_liver:\n",
        "            best_metric_liver = metric_liver\n",
        "            best_metric_epoch_liver = epoch + 1\n",
        "        #     torch.save(model_liver.state_dict(), os.path.join(\n",
        "        #         root_dir, \"best_metric_model_liver_128.pth\"))\n",
        "        #     print(\"saved new best metric model for liver dataset\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current mean dice for liver: {metric_liver:.4f}\"\n",
        "                f\"\\nbest mean dice for liver: {best_metric_liver:.4f} \"\n",
        "                f\"at epoch: {best_metric_epoch_liver}\"\n",
        "            )\n",
        "\n",
        "        # Validation forward Pancreas\n",
        "        for val_data_pan in val_loader_pan:\n",
        "            val_inputs_pan, val_labels_pan = (\n",
        "                val_data_pan[\"image\"].to(device),\n",
        "                val_data_pan[\"label\"].to(device),\n",
        "            )\n",
        "            roi_size = (160, 160, 160)\n",
        "            sw_batch_size = 4\n",
        "            val_outputs_pan = sliding_window_inference(\n",
        "                val_inputs_pan, roi_size, sw_batch_size, model_pan)\n",
        "            val_outputs_pan = [post_pred_pan(i) for i in decollate_batch(val_outputs_pan)]\n",
        "            val_labels_pan = [post_label_pan(i) for i in decollate_batch(val_labels_pan)]\n",
        "            # compute metric for current iteration\n",
        "            dice_metric_pan(y_pred=val_outputs_pan, y=val_labels_pan)\n",
        "\n",
        "        # aggregate the final mean dice result\n",
        "        metric_pan = dice_metric_pan.aggregate().item()\n",
        "        wandb.log({\"val/dice_metric Pancreas\": metric_pan})\n",
        "\n",
        "        scheduler_pan.step(metric_pan)\n",
        "        # reset the status for next validation round\n",
        "        dice_metric_pan.reset()\n",
        "        metric_values_pan.append(metric_pan)\n",
        "        if metric_pan > best_metric_pan:\n",
        "            best_metric_pan = metric_pan\n",
        "            best_metric_epoch_pan = epoch + 1\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current mean dice for pancreas: {metric_pan:.4f}\"\n",
        "                f\"\\nbest mean dice for pancreas: {best_metric_pan:.4f} \"\n",
        "                f\"at epoch: {best_metric_epoch_pan}\"\n",
        "            )\n",
        "\n",
        "\n",
        "        # Validation forward KITS\n",
        "        for val_data_kits in val_loader_kits:\n",
        "            val_inputs_kits, val_labels_kits = (\n",
        "                val_data_kits[\"image\"].to(device),\n",
        "                val_data_kits[\"label\"].to(device),\n",
        "            )\n",
        "            roi_size = (160, 160, 160)\n",
        "            sw_batch_size = 4\n",
        "            val_outputs_kits = sliding_window_inference(\n",
        "                val_inputs_kits, roi_size, sw_batch_size, model_kits)\n",
        "            val_outputs_kits = [post_pred_kits(i) for i in decollate_batch(val_outputs_kits)]\n",
        "            val_labels_kits = [post_label_kits(i) for i in decollate_batch(val_labels_kits)]\n",
        "            # compute metric for current iteration\n",
        "            dice_metric_kits(y_pred=val_outputs_kits, y=val_labels_kits)\n",
        "\n",
        "        # aggregate the final mean dice result\n",
        "        metric_kits = dice_metric_kits.aggregate().item()\n",
        "        wandb.log({\"val/dice_metric KITS\": metric_kits})\n",
        "\n",
        "        scheduler_kits.step(metric_kits)\n",
        "        # reset the status for next validation round\n",
        "        dice_metric_kits.reset()\n",
        "        metric_values_kits.append(metric_kits)\n",
        "        if metric_kits > best_metric_kits:\n",
        "            best_metric_kits = metric_kits\n",
        "            best_metric_epoch_kits = epoch + 1\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current mean dice for kidneys: {metric_kits:.4f}\"\n",
        "                f\"\\nbest mean dice for kidneys: {best_metric_kits:.4f} \"\n",
        "                f\"at epoch: {best_metric_epoch_kits}\"\n",
        "            )\n",
        "\n",
        "        avg_metric = (metric_pan + metric_spleen + metric_liver + metric_kits) / 4\n",
        "\n",
        "        metric_values.append(avg_metric)\n",
        "\n",
        "        if avg_metric > best_metric:\n",
        "            best_metric = avg_metric\n",
        "            best_metric_epoch = epoch + 1\n",
        "            torch.save(model_pan.state_dict(), os.path.join(\n",
        "                root_dir, \"best_metric_model_pan_128_segviz_LSPK.pth\"))\n",
        "            torch.save(model_liver.state_dict(), os.path.join(\n",
        "                root_dir, \"best_metric_model_liver_128_segviz_LSPK.pth\"))\n",
        "            torch.save(model_spleen.state_dict(), os.path.join(\n",
        "                root_dir, \"best_metric_model_spleen_128_segviz_LSPK.pth\"))\n",
        "            torch.save(model_kits.state_dict(), os.path.join(\n",
        "                root_dir, \"best_metric_model_kits_128_segviz_LSPK.pth\"))\n",
        "            print(\"saved new best metric model all datasets\")\n",
        "        \n",
        "\n",
        "\n",
        "wandb.log({\"best_dice_metric spleen\": best_metric_spleen, \"best_metric_epoch spleen\": best_metric_epoch_spleen})\n",
        "wandb.log({\"best_dice_metric liver\": best_metric_liver, \"best_metric_epoch liver\": best_metric_epoch_liver})\n",
        "wandb.log({\"best_dice_metric pancreas\": best_metric_pan, \"best_metric_epoch pancreas\": best_metric_epoch_pan})\n",
        "wandb.log({\"best_dice_metric kidneys\": best_metric_kits, \"best_metric_epoch kidneys\": best_metric_epoch_kits})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTj6tOLTIQNb"
      },
      "source": [
        "# Finetuning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er6gOljSIQNb"
      },
      "outputs": [],
      "source": [
        "model_spleen.load_state_dict(torch.load(\n",
        "    os.path.join(root_dir, \"best_metric_model_spleen_128_segviz_LSPK.pth\")))\n",
        "\n",
        "model_liver.load_state_dict(torch.load(\n",
        "    os.path.join(root_dir, \"best_metric_model_liver_128_segviz_LSPK.pth\")))\n",
        "\n",
        "model_pan.load_state_dict(torch.load(\n",
        "    os.path.join(root_dir, \"best_metric_model_pan_128_segviz_LSPK.pth\")))\n",
        "\n",
        "model_kits.load_state_dict(torch.load(\n",
        "    os.path.join(root_dir, \"best_metric_model_kits_128_segviz_LSPK.pth\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIizE-BuIQNc"
      },
      "outputs": [],
      "source": [
        "# Freeze model layers\n",
        "\n",
        "params_to_train = ['model.2.0.conv.weight',\n",
        "'model.2.0.conv.bias',\n",
        "'model.2.0.adn.N.weight',\n",
        "'model.2.0.adn.N.bias',\n",
        "'model.2.0.adn.A.weight',\n",
        "'model.2.1.conv.unit0.conv.weight',\n",
        "'model.2.1.conv.unit0.conv.bias']\n",
        "\n",
        "for name, param in model_spleen.named_parameters():\n",
        "    param.requires_grad = True if name in params_to_train else False\n",
        "\n",
        "\n",
        "for name, param in model_liver.named_parameters():\n",
        "    param.requires_grad = True if name in params_to_train else False\n",
        "\n",
        "\n",
        "for name, param in model_pan.named_parameters():\n",
        "    param.requires_grad = True if name in params_to_train else False\n",
        "\n",
        "for name, param in model_kits.named_parameters():\n",
        "    param.requires_grad = True if name in params_to_train else False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gJcDO0MIQNc"
      },
      "outputs": [],
      "source": [
        "max_epochs = 150\n",
        "val_interval = 2\n",
        "best_metric = -1\n",
        "best_metric_spleen = -1\n",
        "best_metric_liver = -1\n",
        "best_metric_pan = -1\n",
        "best_metric_kits = -1\n",
        "\n",
        "best_metric_epoch = -1\n",
        "best_metric_epoch_spleen = -1\n",
        "best_metric_epoch_liver = -1\n",
        "best_metric_epoch_pan = -1\n",
        "best_metric_epoch_kits = -1\n",
        "\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "\n",
        "epoch_loss_values_spleen = []\n",
        "metric_values_spleen = []\n",
        "\n",
        "epoch_loss_values_liver = []\n",
        "metric_values_liver = []\n",
        "\n",
        "epoch_loss_values_pan = []\n",
        "metric_values_pan = []\n",
        "\n",
        "epoch_loss_values_kits = []\n",
        "metric_values_kits = []\n",
        "\n",
        "post_pred_spleen = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label_spleen = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "post_pred_liver = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label_liver = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "post_pred_pan = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label_pan = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "post_pred_kits = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label_kits = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "  epoch_loss_spleen = 0\n",
        "  epoch_loss_liver = 0\n",
        "  epoch_loss_pan = 0\n",
        "  epoch_loss_kits = 0\n",
        "\n",
        "  step_0 = 0\n",
        "  step_1 = 0\n",
        "  \n",
        "  # For one epoch\n",
        "  \n",
        "  print(\"-\" * 10)\n",
        "  print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "  \n",
        "  model_spleen.train()\n",
        "  model_liver.train()\n",
        "  model_pan.train()\n",
        "  model_kits.train()\n",
        "  \n",
        "  # One forward pass of the spleen data through the spleen UNet\n",
        "  for batch_data_spleen in train_loader_spleen:\n",
        "      step_0 += 1\n",
        "      inputs_spleen, labels_spleen = (\n",
        "          batch_data_spleen[\"image\"].to(device),\n",
        "          batch_data_spleen[\"label\"].to(device),\n",
        "      )\n",
        "      optimizer_spleen.zero_grad()\n",
        "      outputs_spleen = model_spleen(inputs_spleen)\n",
        "      \n",
        "      loss_spleen = loss_function_spleen(outputs_spleen, labels_spleen)\n",
        "      loss_spleen.backward()\n",
        "      \n",
        "      optimizer_spleen.step()\n",
        "      epoch_loss_spleen += loss_spleen.item()\n",
        "      print(\n",
        "          f\"{step_0}/{len(train_ds_spleen) // train_loader_spleen.batch_size}, \"\n",
        "          f\"train_loss: {loss_spleen.item():.4f}\")\n",
        "    #   wandb.log({\"train/loss spleen\": loss_spleen.item()})\n",
        "  epoch_loss_spleen /= step_0\n",
        "  epoch_loss_values_spleen.append(epoch_loss_spleen)\n",
        "  print(f\"epoch {epoch + 1} average loss spleen: {epoch_loss_spleen:.4f}\")\n",
        "\n",
        "\n",
        "  # One forward pass of the liver data through the liver UNet\n",
        "  for batch_data_liver in train_loader_liver:\n",
        "      step_1 += 1\n",
        "      inputs_liver, labels_liver = (\n",
        "          batch_data_liver[\"image\"].to(device),\n",
        "          batch_data_liver[\"label\"].to(device),\n",
        "      )\n",
        "      optimizer_liver.zero_grad()\n",
        "      outputs_liver = model_liver(inputs_liver)\n",
        "      loss_liver = loss_function_liver(outputs_liver, labels_liver)\n",
        "      loss_liver.backward()\n",
        "      optimizer_liver.step()\n",
        "      epoch_loss_liver += loss_liver.item()\n",
        "      print(\n",
        "          f\"{step_1}/{len(train_ds_liver) // train_loader_liver.batch_size}, \"\n",
        "          f\"train_loss: {loss_liver.item():.4f}\")\n",
        "    #   wandb.log({\"train/loss liver\": loss_liver.item()})\n",
        "  epoch_loss_liver /= step_1\n",
        "  epoch_loss_values_liver.append(epoch_loss_liver)\n",
        "  print(f\"epoch {epoch + 1} average loss liver: {epoch_loss_liver:.4f}\")\n",
        "\n",
        "   # One forward pass of the liver data through the Pancreas UNet\n",
        "  for batch_data_pan in train_loader_pan:\n",
        "      step_1 += 1\n",
        "      inputs_pan, labels_pan = (\n",
        "          batch_data_pan[\"image\"].to(device),\n",
        "          batch_data_pan[\"label\"].to(device),\n",
        "      )\n",
        "      optimizer_pan.zero_grad()\n",
        "      outputs_pan = model_pan(inputs_pan)\n",
        "      loss_pan = loss_function_pan(outputs_pan, labels_pan)\n",
        "      loss_pan.backward()\n",
        "      optimizer_pan.step()\n",
        "      epoch_loss_pan += loss_pan.item()\n",
        "      print(\n",
        "          f\"{step_1}/{len(train_ds_pan) // train_loader_pan.batch_size}, \"\n",
        "          f\"train_loss: {loss_pan.item():.4f}\")\n",
        "    #   wandb.log({\"train/loss pancreas\": loss_pan.item()})\n",
        "  epoch_loss_pan /= step_1\n",
        "  epoch_loss_values_pan.append(epoch_loss_pan)\n",
        "  print(f\"epoch {epoch + 1} average loss pancreas: {epoch_loss_pan:.4f}\")\n",
        "  \n",
        "  for batch_data_kits in train_loader_kits:\n",
        "      step_1 += 1\n",
        "      inputs_kits, labels_kits = (\n",
        "          batch_data_kits[\"image\"].to(device),\n",
        "          batch_data_kits[\"label\"].to(device),\n",
        "      )\n",
        "      optimizer_kits.zero_grad()\n",
        "      outputs_kits = model_kits(inputs_kits)\n",
        "      loss_kits = loss_function_kits(outputs_kits, labels_kits)\n",
        "      loss_kits.backward()\n",
        "      optimizer_kits.step()\n",
        "      epoch_loss_kits += loss_kits.item()\n",
        "      print(\n",
        "          f\"{step_1}/{len(train_ds_kits) // train_loader_kits.batch_size}, \"\n",
        "          f\"train_loss: {loss_kits.item():.4f}\")\n",
        "    #   wandb.log({\"train/loss kidneys\": loss_kits.item()})\n",
        "  epoch_loss_kits /= step_1\n",
        "  epoch_loss_values_kits.append(epoch_loss_kits)\n",
        "  print(f\"epoch {epoch + 1} average loss kidneys: {epoch_loss_kits:.4f}\")\n",
        "\n",
        "  scheduler_spleen.step()\n",
        "  scheduler_liver.step()\n",
        "  scheduler_pan.step()\n",
        "  scheduler_kits.step()\n",
        "\n",
        "  # Validation \n",
        "  if (epoch + 1) % val_interval == 0:\n",
        "    model_spleen.eval()\n",
        "    model_liver.eval()\n",
        "    model_pan.eval()\n",
        "    model_kits.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Validation forward spleen\n",
        "        for val_data_spleen in val_loader_spleen:\n",
        "            val_inputs_spleen, val_labels_spleen = (\n",
        "                val_data_spleen[\"image\"].to(device),\n",
        "                val_data_spleen[\"label\"].to(device),\n",
        "            )\n",
        "            roi_size = (160, 160, 160)\n",
        "            sw_batch_size = 4\n",
        "            val_outputs_spleen = sliding_window_inference(\n",
        "                val_inputs_spleen, roi_size, sw_batch_size, model_spleen)\n",
        "            val_outputs_spleen = [post_pred_spleen(i) for i in decollate_batch(val_outputs_spleen)]\n",
        "            val_labels_spleen = [post_label_spleen(i) for i in decollate_batch(val_labels_spleen)]\n",
        "            # compute metric for current iteration\n",
        "            dice_metric_spleen(y_pred=val_outputs_spleen, y=val_labels_spleen)\n",
        "\n",
        "        # aggregate the final mean dice result\n",
        "        metric_spleen = dice_metric_spleen.aggregate().item()\n",
        "        # wandb.log({\"val/dice_metric spleen\": metric_spleen})\n",
        "        scheduler_spleen.step(metric_spleen)\n",
        "        # reset the status for next validation round\n",
        "        dice_metric_spleen.reset()\n",
        "\n",
        "        metric_values_spleen.append(metric_spleen)\n",
        "        if metric_spleen > best_metric_spleen:\n",
        "            best_metric_spleen = metric_spleen\n",
        "            best_metric_epoch_spleen = epoch + 1\n",
        "            torch.save(model_spleen.state_dict(), os.path.join(\n",
        "                root_dir, \"best_metric_model_spleen_128segviz_finetuned_LSPK.pth\"))\n",
        "            print(\"saved new best metric model for spleen dataset\")\n",
        "        print(\n",
        "            f\"current epoch: {epoch + 1} current mean dice for spleen: {metric_spleen:.4f}\"\n",
        "            f\"\\nbest mean dice for spleen: {best_metric_spleen:.4f} \"\n",
        "            f\"at epoch: {best_metric_epoch_spleen}\"\n",
        "        )\n",
        "\n",
        "\n",
        "        # Validation forward Liver \n",
        "        for val_data_liver in val_loader_liver:\n",
        "            val_inputs_liver, val_labels_liver = (\n",
        "                val_data_liver[\"image\"].to(device),\n",
        "                val_data_liver[\"label\"].to(device),\n",
        "            )\n",
        "            roi_size = (160, 160, 160)\n",
        "            sw_batch_size = 4\n",
        "            val_outputs_liver = sliding_window_inference(\n",
        "                val_inputs_liver, roi_size, sw_batch_size, model_liver)\n",
        "            val_outputs_liver = [post_pred_liver(i) for i in decollate_batch(val_outputs_liver)]\n",
        "            val_labels_liver = [post_label_liver(i) for i in decollate_batch(val_labels_liver)]\n",
        "            # compute metric for current iteration\n",
        "            dice_metric_liver(y_pred=val_outputs_liver, y=val_labels_liver)\n",
        "\n",
        "        # aggregate the final mean dice result\n",
        "        metric_liver = dice_metric_liver.aggregate().item()\n",
        "        # wandb.log({\"val/dice_metric liver\": metric_liver})\n",
        "\n",
        "        scheduler_liver.step(metric_liver)\n",
        "        # reset the status for next validation round\n",
        "        dice_metric_liver.reset()\n",
        "\n",
        "        metric_values_liver.append(metric_liver)\n",
        "        if metric_liver > best_metric_liver:\n",
        "            best_metric_liver = metric_liver\n",
        "            best_metric_epoch_liver = epoch + 1\n",
        "            torch.save(model_liver.state_dict(), os.path.join(\n",
        "                root_dir, \"best_metric_model_liver_128segviz_finetuned_LSPK.pth\"))\n",
        "            print(\"saved new best metric model for liver dataset\")\n",
        "        print(\n",
        "            f\"current epoch: {epoch + 1} current mean dice for liver: {metric_liver:.4f}\"\n",
        "            f\"\\nbest mean dice for liver: {best_metric_liver:.4f} \"\n",
        "            f\"at epoch: {best_metric_epoch_liver}\"\n",
        "        )\n",
        "\n",
        "\n",
        "        # Validation forward KITS \n",
        "        for val_data_kits in val_loader_kits:\n",
        "            val_inputs_kits, val_labels_kits = (\n",
        "                val_data_kits[\"image\"].to(device),\n",
        "                val_data_kits[\"label\"].to(device),\n",
        "            )\n",
        "            roi_size = (160, 160, 160)\n",
        "            sw_batch_size = 4\n",
        "            val_outputs_kits = sliding_window_inference(\n",
        "                val_inputs_kits, roi_size, sw_batch_size, model_kits)\n",
        "            val_outputs_kits = [post_pred_kits(i) for i in decollate_batch(val_outputs_kits)]\n",
        "            val_labels_kits = [post_label_kits(i) for i in decollate_batch(val_labels_kits)]\n",
        "            # compute metric for current iteration\n",
        "            dice_metric_kits(y_pred=val_outputs_kits, y=val_labels_kits)\n",
        "\n",
        "        # aggregate the final mean dice result\n",
        "        metric_kits = dice_metric_kits.aggregate().item()\n",
        "        # wandb.log({\"val/dice_metric KITS\": metric_kits})\n",
        "\n",
        "        scheduler_kits.step(metric_kits)\n",
        "        # reset the status for next validation round\n",
        "        dice_metric_kits.reset()\n",
        "        metric_values_kits.append(metric_kits)\n",
        "        if metric_kits > best_metric_kits:\n",
        "            best_metric_kits = metric_kits\n",
        "            best_metric_epoch_kits = epoch + 1\n",
        "            torch.save(model_kits.state_dict(), os.path.join(\n",
        "                root_dir, \"best_metric_model_kits_128segviz_finetuned_LSPK.pth\"))\n",
        "            print(\"saved new best metric model for KITS dataset\")\n",
        "        print(\n",
        "            f\"current epoch: {epoch + 1} current mean dice for KITS: {metric_kits:.4f}\"\n",
        "            f\"\\nbest mean dice for KITS: {best_metric_kits:.4f} \"\n",
        "            f\"at epoch: {best_metric_epoch_kits}\"\n",
        "        )\n",
        "\n",
        "\n",
        "        # Validation forward Pancreas\n",
        "        for val_data_pan in val_loader_pan:\n",
        "            val_inputs_pan, val_labels_pan = (\n",
        "                val_data_pan[\"image\"].to(device),\n",
        "                val_data_pan[\"label\"].to(device),\n",
        "            )\n",
        "            roi_size = (160, 160, 160)\n",
        "            sw_batch_size = 4\n",
        "            val_outputs_pan = sliding_window_inference(\n",
        "                val_inputs_pan, roi_size, sw_batch_size, model_pan)\n",
        "            val_outputs_pan = [post_pred_pan(i) for i in decollate_batch(val_outputs_pan)]\n",
        "            val_labels_pan = [post_label_pan(i) for i in decollate_batch(val_labels_pan)]\n",
        "            # compute metric for current iteration\n",
        "            dice_metric_pan(y_pred=val_outputs_pan, y=val_labels_pan)\n",
        "\n",
        "        # aggregate the final mean dice result\n",
        "        metric_pan = dice_metric_pan.aggregate().item()\n",
        "        # wandb.log({\"val/dice_metric Pancreas\": metric_pan})\n",
        "\n",
        "        scheduler_pan.step(metric_pan)\n",
        "        # reset the status for next validation round\n",
        "        dice_metric_pan.reset()\n",
        "\n",
        "        avg_metric = (metric_pan + metric_spleen + metric_liver) / 3\n",
        "\n",
        "        metric_values.append(avg_metric)\n",
        "        if metric_pan > best_metric_pan:\n",
        "            best_metric_pan = metric_pan\n",
        "            best_metric_epoch_pan = epoch + 1\n",
        "            torch.save(model_pan.state_dict(), os.path.join(\n",
        "                root_dir, \"best_metric_model_pan_128segviz_finetuned_LSPK.pth\"))\n",
        "            print(\"saved new best metric model for liver dataset\")\n",
        "        print(\n",
        "            f\"current epoch: {epoch + 1} current mean dice for pancreas: {metric_pan:.4f}\"\n",
        "            f\"\\nbest mean dice for pancreas: {best_metric_pan:.4f} \"\n",
        "            f\"at epoch: {best_metric_epoch_pan}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEuTuXmaPg0o"
      },
      "source": [
        "## Plot the loss and metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csK69MIDPg0o"
      },
      "outputs": [],
      "source": [
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Epoch Average Loss\")\n",
        "x = [i + 1 for i in range(len(epoch_loss_values_spleen))]\n",
        "y = epoch_loss_values_spleen\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values_spleen))]\n",
        "y = metric_values_spleen\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_rd2Ke-j2ZP"
      },
      "outputs": [],
      "source": [
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Epoch Average Loss\")\n",
        "x = [i + 1 for i in range(len(epoch_loss_values_liver))]\n",
        "y = epoch_loss_values_liver\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values_liver))]\n",
        "y = metric_values_liver\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85nEgIkIN6B6"
      },
      "outputs": [],
      "source": [
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Epoch Average Loss\")\n",
        "x = [i + 1 for i in range(len(epoch_loss_values_pan))]\n",
        "y = epoch_loss_values_pan\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values_pan))]\n",
        "y = metric_values_pan\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69Acb5RKJH0F"
      },
      "outputs": [],
      "source": [
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Epoch Average Loss\")\n",
        "x = [i + 1 for i in range(len(epoch_loss_values_kits))]\n",
        "y = epoch_loss_values_kits\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values_kits))]\n",
        "y = metric_values_kits\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "0ca5fdb1364b7850985fadde1ce5cdfbf8a36ab9b8a2920002462139b38e4875"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
